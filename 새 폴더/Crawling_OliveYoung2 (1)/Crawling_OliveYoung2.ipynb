{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "stopWord = ['ㅋ',',','^','ㅠ','ㅡ','ㅎ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_data():\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(\"data/naver_shopping.txt\",delimiter='    ',encoding='utf-8',names=['내용'])\n",
    "    data=df.values\n",
    "    a=df['내용'].tolist()\n",
    "    result=[]\n",
    "    for i in a:\n",
    "        temp = i.split(\"\\t\")\n",
    "        result.append(temp)\n",
    "    result\n",
    "    for j in result:\n",
    "        if int(j[0])>=3:\n",
    "            j[0]=1\n",
    "        else:\n",
    "            j[0]=0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/naver_shopping.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9f5b2f7b5a57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawling_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-75c7e964252b>\u001b[0m in \u001b[0;36mcrawling_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcrawling_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/naver_shopping.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'    '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'내용'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'내용'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                     \u001b[1;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m                 )\n\u001b[1;32m-> 1179\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   2375\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2376\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2377\u001b[1;33m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2378\u001b[0m         )\n\u001b[0;32m   2379\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/naver_shopping.txt'"
     ]
    }
   ],
   "source": [
    "result = crawling_data()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize_and_Posing(text):\n",
    "    # INPUT = string\n",
    "    \n",
    "    from konlpy.tag import Mecab\n",
    "    mecab = Mecab()\n",
    "    mecab_tokenized = mecab.pos(text)\n",
    "    \n",
    "    # OUTPUT = list\n",
    "    return mecab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(result)\n",
    "data1 = list(data[1])\n",
    "data1\n",
    "data2 = list(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data1\n",
    "y_train = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from konlpy.tag import Mecab\n",
    "import nltk\n",
    "stopWord = ['ㅋ',',','^','ㅠ','ㅡ','ㅎ']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# CountVectorizer 선언\n",
    "count_vect = CountVectorizer(stop_words=stopWord, tokenizer=nltk.word_tokenize)\n",
    "# fit and transform\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "# MultinomialNB 선언 and fit\n",
    "clf = MultinomialNB().fit(x_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'model/finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from konlpy.tag import Mecab\n",
    "import nltk\n",
    "stopWord = ['ㅋ',',','^','ㅠ','ㅡ','ㅎ']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# CountVectorizer 선언\n",
    "count_vect = CountVectorizer(stop_words=stopWord, tokenizer=nltk.word_tokenize)\n",
    "clf = MultinomialNB().fit(count_vect.fit_transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'model/a_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "pd_e = count_vect.transform(['거지같아'])\n",
    "result = loaded_model.predict(pd_e)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "save_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 카테고리 제품 ID list 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 카테고리 제품 ID list 획득\n",
    "def get_items(category):\n",
    "    page_num = 2 # 사용자 지정\n",
    "    result = []\n",
    "    base_url = 'https://www.oliveyoung.co.kr/store/display/getMCategoryList.do'\n",
    "    url = 'https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=1000001000100010001&fltDispCatNo=&prdSort=01&pageIdx=2&rowsPerPage=24&searchTypeSort=btn_thumb&plusButtonFlag=N&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0'\n",
    "    for i in range(page_num):\n",
    "        params = {\n",
    "            'dispCatNo': category, # 카테고리 ID\n",
    "            'prdSort': '01', # 인기순\n",
    "            'pageIdx': i + 1, # 페이지 번호\n",
    "            'rowsPerPage': 24 # 한 페이지에 표시할 상품수\n",
    "        }\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'\n",
    "        }\n",
    "        resp = requests.get(base_url, params=params, headers=headers)\n",
    "        print(resp)\n",
    "        soup = BeautifulSoup(resp.text)\n",
    "        prd_list = soup.find_all('a', class_='prd_thumb goodsList')\n",
    "\n",
    "        for prd in prd_list:\n",
    "            result.append(prd.get('data-ref-goodsno'))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제품 정보 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 카테고리 제품들 정보 획득\n",
    "def get_items_info(prd_list, category):\n",
    "    prd_url = 'https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do'\n",
    "    result = []\n",
    "\n",
    "    for prd_id in prd_list:\n",
    "        temp = {}\n",
    "        params = {\n",
    "            'goodsNo': prd_id, # 제품 ID\n",
    "            'dispCatNo': category # 카테고리 ID\n",
    "        }\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'\n",
    "        }\n",
    "        resp = requests.get(prd_url, params=params, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text)\n",
    "\n",
    "        info = soup.find('div', class_='prd_info')\n",
    "        prd_name = info.find('p', class_='prd_name').text\n",
    "        prd_price = info.find('span', class_='price-2').find('strong').text\n",
    "        \n",
    "        tag_list = get_tags(prd_id, count_vect) # 제품의 리뷰 태그 리스트\n",
    "        count_list = tag_count(tag_list) # 태그별 출현 횟수\n",
    "        top_tags = max_pooling(count_list) # 빈도수가 가장 많은 태그 추출\n",
    "        \n",
    "        top_tags_str = \"\"\n",
    "        for tag in top_tags: # 태그리스트를 문자열로 변환 => csv 저장시 필요\n",
    "            top_tags_str = top_tags_str + tag + ' '\n",
    "        \n",
    "        \n",
    "        prd_info = { # 제품 정보\n",
    "            'id': prd_id, # ID\n",
    "            'name': prd_name, # 이름\n",
    "            'price': prd_price, # 가격\n",
    "            'top_tags': top_tags_str # 상위 6개 태그\n",
    "        }\n",
    "        \n",
    "        result.append(prd_info)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제품 리뷰 태그 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제품 리뷰 태그 획득 => 태그(필터): 피부타입(지성,복합성,...), 피부톤(...), 피부고민(...)\n",
    "def get_tags(prd_id, count_vect):\n",
    "    result = []\n",
    "    i = 0\n",
    "    review_url = 'https://www.oliveyoung.co.kr/store/goods/getGdasNewListJson.do'\n",
    "    filename = 'model/finalized_model.sav'  # 긍부정 판별 모델  # 파일 경로 및 이름 확인 필요\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    while True:\n",
    "        i += 1\n",
    "        params = {\n",
    "            'goodsNo': prd_id, # 제품 ID\n",
    "            'gdasSort': '05', # 정렬: 유용한순\n",
    "            'pageIdx': i # 페이지 번호\n",
    "        }\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'\n",
    "        }\n",
    "        resp = requests.get(review_url, params=params, headers=headers)\n",
    "        review_data = resp.json() # jason data 저장\n",
    "        review_list = review_data['gdasList'] # 리뷰 데이터 리스트 (한 페이지당 최대 10개의 리뷰 존재)\n",
    "        \n",
    "        if len(review_list) == 0: # 마지막 페이지를 넘긴 경우 while문 종료\n",
    "            break\n",
    "        \n",
    "        for review in review_list:\n",
    "            ################## new: use model ##################\n",
    "            comment = review['gdasCont'] # 리뷰 내용\n",
    "            if comment is not None:\n",
    "                loaded_model = pickle.load(open(filename, 'rb'))\n",
    "                pd_e = count_vect.transform(list(comment))\n",
    "                predict = loaded_model.predict(pd_e) # 댓글 긍/부정 예측\n",
    "\n",
    "                if predict[0] == 1: # 긍정 리뷰의 데이터만 저장\n",
    "\n",
    "                ################## old : use score ##################\n",
    "                # score = int(review['gdasScrVal']) / 2 # 리뷰 별점\n",
    "                # if score >= 5: # 별점 5개짜리 리뷰 정보만 저장\n",
    "                    try:\n",
    "                        tags= []\n",
    "                        for info in review[\"addInfoNm\"]: # 해당 리뷰에 존재하는 모든 태그 저장\n",
    "                            tags.append(info['mrkNm'])\n",
    "                        result.append(tags)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그별 출현 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그별 출현 횟수 \n",
    "# 기존 preprocessing\n",
    "def tag_count(tag_list):\n",
    "    # INPUT EX)\n",
    "    #     [\n",
    "    #         [\"복합성\",\"쿨톤\",\"잡티\",\"미백\",\"주름\",\"각질\"],\n",
    "    #         [\"건성\",\"웜톤\",\"트러블\",\"블랙헤드\"], ...\n",
    "    #     ]\n",
    "    # base_list : Back Of Words\n",
    "    name_list = [\"복합성\",\"건성\",\"지성\",\"쿨톤\",\"웜톤\",\"잡티\",\"미백\",\"주름\",\"각질\",\"트러블\",\"블랙헤드\", \\\n",
    "                 \"피지과다\",\"민감성\",\"모공\",\"탄력\",\"홍조\",\"아토피\"]\n",
    "    result = [0 for i in range(len(name_list))] # zeros matrix for counting frequency\n",
    "    for tags in tag_list:\n",
    "        for tag in tags:\n",
    "            index = name_list.index(tag)\n",
    "            result[index] += 1\n",
    "    # 단어수를 세서 반환\n",
    "    # OUTPUT EX) [7, 20, 3, 10, 19, 14, 13, 15, 11, 15, 12, 6, 16, 13, 14, 6, 3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도수가 가장 많은 태그 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(tag_count):\n",
    "    #카테고리별로 빈도수가 가장 많은 키워드 추출\n",
    "    # 피부 성격 = [\"복합성\",\"건성\",\"지성\"] 중에 1개\n",
    "    # 피부 빛 = [\"쿨톤\",\"웜톤\"] 중에 1개\n",
    "    # 피부 트러블 = [\"잡티\",\"미백\",\"주름\",\"각질\",\"트러블\",\"블랙헤드\",\"피지과다\",\"민감성\",\"모공\",\"탄력\",\"홍조\",\"아토피\"] 중에 4개\n",
    "    import random\n",
    "    \n",
    "    name_list=[\"복합성\",\"건성\",\"지성\",\"쿨톤\",\"웜톤\",\"잡티\",\"미백\",\"주름\",\"각질\",\"트러블\",\"블랙헤드\", \\\n",
    "               \"피지과다\",\"민감성\",\"모공\",\"탄력\",\"홍조\",\"아토피\"]\n",
    "    \n",
    "    try:\n",
    "        max_1=max(tag_count[0:3])\n",
    "        index_1 = tag_count[0:3].index(max_1)\n",
    "        sol_1 = name_list[index_1]\n",
    "    except:\n",
    "        sol_1='복합성'\n",
    "    \n",
    "    try:\n",
    "        max_2=max(tag_count[3:5])\n",
    "        index_2 = tag_count[3:5].index(max_2)+3\n",
    "        sol_2 = name_list[index_2]\n",
    "    except:\n",
    "        sol_2='웜톤'\n",
    "    result=[]\n",
    "    result.append(sol_1)\n",
    "    result.append(sol_2)\n",
    "    temp_list=[]\n",
    "    for i in zip(name_list[5:-1],tag_count[5:-1]):\n",
    "        temp_list.append(i)\n",
    "    new_temp_list = sorted(temp_list, key=lambda temp_list: temp_list[1],reverse=True)\n",
    "\n",
    "    for k in range(4):\n",
    "        try:    \n",
    "            result.append(new_temp_list[k][0])\n",
    "        except:\n",
    "            shuffle_list = random.shuffle(name_list[5:-1])\n",
    "            result.extend(shuffle_list[0:4])\n",
    "    #OUTPUT EX)\n",
    "    #   ['건성', '웜톤', '민감성', '주름', '트러블', '잡티']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일로 저장\n",
    "def save(category, result):\n",
    "    import pandas as pd\n",
    "    file_name = 'data/' + category + '.csv'\n",
    "    data = pd.DataFrame(result)    \n",
    "    data.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 저장\n",
    "def save_all():\n",
    "#     category_ids = ['1000001000100010001', '1000001000100010002', '1000001000100010003', \\\n",
    "#                     '1000001000100010011', '1000001000100010004']\n",
    "\n",
    "    category_ids = ['1000001000100010001'] # test용\n",
    "    for category in category_ids:\n",
    "        prd_list = get_items(category)\n",
    "        result = get_items_info(prd_list, category)\n",
    "        save(category, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카테고리 코드별 이름\n",
    "\n",
    "category_name = ['스킨/토너', '로션', '에센스/세럼', '앰플', '크림']\n",
    "\n",
    "category_ids = ['1000001000100010001', '1000001000100010002', '1000001000100010003', '1000001000100010011', '1000001000100010004']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수정 필요\n",
    "import \n",
    "\n",
    "get_tags 함수 => model 파일명, 벡터라이징 함수\n",
    "\n",
    "벡터라이징 함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "save_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
